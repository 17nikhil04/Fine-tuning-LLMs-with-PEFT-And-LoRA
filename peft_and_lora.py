# -*- coding: utf-8 -*-
"""peft and loRA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EVYf4Vq2lMv-7Sc9uG8HwJgp36fSg4PK
"""



!pip install transformers datasets evaluate rouge_score loralib peft

from datasets import load_dataset
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer
import torch
import time
import evaluate
import pandas as pd
import numpy as np

pip install awscli

!pip install -U datasets huggingface_hub fsspec
!pip install transformers evaluate rouge_score loralib peft

huggingface_dataset_name = "knkarthick/dialogsum"
dataset = load_dataset(huggingface_dataset_name)
dataset

dataset.data

model_name='google/flan-t5-base'
original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)
tokenizer = AutoTokenizer.from_pretrained(model_name)

dataset['test'][200]['dialogue']

dataset['test'][200]['summary']

index=200
dialogue = dataset['test'][index]['dialogue']
summary = dataset['test'][index]['summary']
prompt = f"""
Summarize the following conversation.

{dialogue}

Summary:
"""
input = tokenizer(prompt, return_tensors='pt')
output = tokenizer.decode(
    original_model.generate(
        input['input_ids'],
        max_new_tokens=200,
    )[0],
    skip_special_tokens=True
)

dash_line = '-' * 100
print(dash_line)
print(f'INPUT PROMPT:\n{prompt}')
print(dash_line)
print(f'BASELINE HUMAN SUMMARY:\n{summary}\n')
print(dash_line)
print(f'MODEL GENERATION - ZERO SHOT:\n{output}')

def tokenizer_function(example):
    start_prompt = 'Summarize the following conversation.\n\n'
    end_prompt = '\n\nSummary: '
    prompt = [start_prompt + dialogue + end_prompt for dialogue in example['dialogue']]
    example['input_ids']= tokenizer(prompt, padding='max_length', truncation=True, return_tensors='pt').input_ids
    example['labels'] = tokenizer(example['summary'], padding='max_length', truncation=True, return_tensors='pt').input_ids
    return example


tokenized_datasets= dataset.map(tokenizer_function, batched=True)
tokenized_datasets= tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])

tokenize_datasets= tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)

print(f'shapes of the datasets:')
print(f'training: {tokenize_datasets["train"].shape}')
print(f'validation: {tokenize_datasets["validation"].shape}')
print(f'test: {tokenize_datasets["test"].shape}')

import os

output_dir = './dialogue-summary-training'
training_args = TrainingArguments(
    output_dir=output_dir,
    learning_rate=1e-5,
    num_train_epochs=1,
    weight_decay=0.01,
    logging_steps=1,
    max_steps=1,
    report_to=None,
)

trainer = Trainer(
    model=original_model,
    args=training_args,
    train_dataset=tokenize_datasets['train'],
    eval_dataset=tokenize_datasets['validation']
)

trainer.train()

# Save the model

trainer.save_model("dialogue-summary-training")

# Save the configuration file
model = trainer.model
model.config.save_pretrained("dialogue-summary-training_config.json")

original_model = original_model.to('cpu')

instruct_model = AutoModelForSeq2SeqLM.from_pretrained("dialogue-summary-training")

index=200
dialogue = dataset['test'][index]['dialogue']
summary = dataset['test'][index]['summary']
prompt = f"""
Summarize the following conversation.

{dialogue}

Summary:
"""
input_ids= tokenizer(prompt, return_tensors='pt').input_ids
#output = tokenizer.decode(
#    original_model.generate(
#        input['input_ids'],
#        max_new_tokens=200,
#    )[0],
#    skip_special_tokens=True
#)
instruct_model_output = instruct_model.generate(input_ids= input_ids,generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))
instruct_model_text_output = tokenizer. decode(instruct_model_output [0], skip_special_tokens=True)

print(dash_line)
print(f'INPUT PROMPT:\n{prompt}')
print(dash_line)
print(f'BASELINE HUMAN SUMMARY:\n{summary}\n')
print(dash_line)
print(f'MODEL GENERATION - ZERO SHOT:\n{output}')
print(dash_line)
print(f'Instruct_model - ZERO SHOT:\n{instruct_model_text_output}')


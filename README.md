# PEFT & LoRA Fine-Tuning on LLaMA-2 using Hugging Face Transformers
This project demonstrates efficient fine-tuning of a large language model (LLaMA-2) using PEFT (Parameter-Efficient Fine-Tuning) techniquesâ€”specifically LoRA (Low-Rank Adaptation). It showcases how to adapt powerful models to custom tasks with minimal resource usage.

